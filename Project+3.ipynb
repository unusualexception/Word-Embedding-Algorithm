{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import nltk\n",
    "import itertools\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import brown, stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class WordEmbeddingsUtil(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_brown_words():\n",
    "        raw_brown_words = [str(word.lower()) for word in brown.words()]\n",
    "        stop_words = [str(word.lower())\n",
    "                      for word in stopwords.words('english')]\n",
    "        stop_words.append(string.punctuation)\n",
    "        return [word for word in raw_brown_words\n",
    "                if word not in stop_words and word.isalnum()]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_common_vocab_words(word_list, common_count, vocab_count):\n",
    "        freq_dist = nltk.FreqDist(word_list)\n",
    "        vocab_count_dict = {}\n",
    "        vocab_words = []\n",
    "        common_tuple = freq_dist.most_common(common_count)\n",
    "        common_words = [tup[0] for tup in common_tuple]\n",
    "        vocab_tuple = freq_dist.most_common(vocab_count)\n",
    "        for tup in vocab_tuple:\n",
    "            vocab_words.append(tup[0])\n",
    "            vocab_count_dict[tup[0]] = tup[1]\n",
    "        return common_words, vocab_words, vocab_count_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_four_grams( word_list, vocab_words):\n",
    "        four_grams = []\n",
    "        all_tuples = zip(word_list, word_list[1:],word_list[2:],word_list[3:],word_list[4:])\n",
    "        for tup in all_tuples:\n",
    "            if tup[2] in vocab_words:\n",
    "                four_grams.append(tup)\n",
    "        return four_grams\n",
    "\n",
    "    @staticmethod\n",
    "    def create_vocab_common_dict( vocab, common):\n",
    "        vocab_common_keys = list(itertools.product(\n",
    "            vocab, common))\n",
    "        return dict.fromkeys(vocab_common_keys, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_vocab_occurrence(vocab_dict, four_gram_tuple, common):\n",
    "        for four_gram in four_gram_tuple:\n",
    "            for word in four_gram:\n",
    "                if word in common:\n",
    "                    vocab_dict[(four_gram[2], word)] += 1.0\n",
    "                    print (\"Found : \" + str((four_gram[2], word)))\n",
    "        return vocab_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def store_dict_file(vocab_dict, filename):\n",
    "        np.save(filename, vocab_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dict_file(filename):\n",
    "        return np.load(filename).item()\n",
    "\n",
    "\n",
    "class WordCluster(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def pca_dim_reduce(phi_vector, reduce_dim):\n",
    "        pca = PCA(n_components=reduce_dim).fit_transform(phi_vector)\n",
    "        return pca\n",
    "\n",
    "    @staticmethod\n",
    "    def kmeans_clustering(vector, cluster_size):\n",
    "        kmeans = KMeans(n_clusters=cluster_size).fit(vector)\n",
    "        return kmeans\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_dist(vector1, vector2):\n",
    "        vector1 = np.asarray(vector1)\n",
    "        vector2 = np.asarray(vector2)\n",
    "        distance = 1.0 - (np.dot(vector1.T, vector2)) / (np.linalg.norm(\n",
    "            vector1) * np.linalg.norm(vector2))\n",
    "        return distance\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    common_count = 1000\n",
    "    vocab_count = 5000\n",
    "    reduced_dim = 100\n",
    "    cluster_size = 100\n",
    "    dict_file = 'vocab_dict.npy'\n",
    "    reuse_prob_dict = True\n",
    "    we = WordEmbeddingsUtil()\n",
    "    wc = WordCluster()\n",
    "\n",
    "    # Get Brown corpus words\n",
    "    print (\"Get Brown corpus words\")\n",
    "    brown_words = we.get_brown_words()\n",
    "\n",
    "    # Get common words and vocabulary words\n",
    "    print (\"Get common words and vocabulary words\")\n",
    "    brown_common, brown_vocab, vocab_count_dict = \\\n",
    "        we.get_common_vocab_words(brown_words, common_count,\n",
    "                                  vocab_count)\n",
    "\n",
    "    if not reuse_prob_dict:\n",
    "        # Build Four-grams from the corpus list\n",
    "        print (\"Build Four-grams from the corpus list\")\n",
    "        brown_four_gram = we.get_four_grams(brown_words, brown_vocab)\n",
    "\n",
    "        #  Create a dictionary from vocabulary and common\n",
    "        print (\"Create a dictionary from vocabulary and common\")\n",
    "        vocab_dict = we.create_vocab_common_dict(brown_vocab, brown_common)\n",
    "\n",
    "        # Populate dictionary with occurrence count\n",
    "        print (\"Populate dictionary with occurrence count\")\n",
    "        vocab_dict = we.find_vocab_occurrence(vocab_dict, brown_four_gram, brown_common)\n",
    "\n",
    "        # Store dictionary in a file for future use\n",
    "        print (\"Storing dictionary in NPY file\")\n",
    "        we.store_dict_file(vocab_dict, dict_file)\n",
    "\n",
    "    else:\n",
    "        # Load dictionary from preprocessed file\n",
    "        # Returns dictionary with { (vocab, common) : [count] }\n",
    "        print (\"Loading dictionary from NPY file\")\n",
    "        vocab_dict = we.load_dict_file(dict_file)\n",
    "\n",
    "    # Compute P(c|w) = N(c,w) / N(w)\n",
    "    print (\"Computing Probability of C given W\")\n",
    "    prob_common_given_vocab = {}\n",
    "    for key in vocab_dict.keys():\n",
    "        prob_common_given_vocab[key] = float(vocab_dict[key]) / float(vocab_count_dict[key[0]])\n",
    "\n",
    "    # Compute P(c) = N(c) / N(total words)\n",
    "    print (\"Computing Probability of C\")\n",
    "    prob_common = {}\n",
    "    for word in brown_common:\n",
    "        prob_common[word] = float(vocab_count_dict[word]) / float(len(brown_words))\n",
    "\n",
    "    # PHI vector for each word\n",
    "    print (\"Building PHI vector for each vocab word\")\n",
    "    phi_vocab = {}\n",
    "    for word in brown_vocab:\n",
    "        phi_vocab[word] = []\n",
    "        for common in brown_common:\n",
    "            if prob_common_given_vocab[(word, common)] == 0:\n",
    "                log_cw_c = 0\n",
    "            else:\n",
    "                log_cw_c = np.log(prob_common_given_vocab[(\n",
    "                    word,common)]/prob_common[common])\n",
    "            phi_vocab[word].append(max(0, log_cw_c))\n",
    "\n",
    "    # Converting dictionary to list for dimensionality reduction\n",
    "    print (\"Converting dictionary to list\")\n",
    "    word_list = []\n",
    "    vector_list = []\n",
    "    for word, vector in phi_vocab.items():\n",
    "        word_list.append(word)\n",
    "        vector_list.append(vector)\n",
    "\n",
    "    # Applying PCA for dimensionality reduction\n",
    "    print (\"Applying PCA\")\n",
    "    reduced_vector = wc.pca_dim_reduce(vector_list, reduced_dim)\n",
    "\n",
    "    # Cluster the reduced vector using K-Means Clustering\n",
    "    print (\"Applying K Means Clustering\")\n",
    "    kmeans = wc.kmeans_clustering(reduced_vector, cluster_size)\n",
    "\n",
    "    # Grouping vocabulary based on clusters\n",
    "    print (\"Grouping vocabulary based on labels\")\n",
    "    word_clusters = {}\n",
    "    for idx in range(len(kmeans.labels_)):\n",
    "        if kmeans.labels_[idx] not in word_clusters.keys():\n",
    "            word_clusters[kmeans.labels_[idx]] = [word_list[idx]]\n",
    "        else:\n",
    "            word_clusters[kmeans.labels_[idx]].append(word_list[idx])\n",
    "\n",
    "    test_words = ['communism', 'autumn', 'cigarette', 'pulmonary',\n",
    "                  'mankind', 'africa', 'chicago', 'revolution', 'september',\n",
    "                  'chemical', 'detergent', 'dictionary', 'storm', 'worship']\n",
    "\n",
    "    # Finding Nearest neighbour for list of test words\n",
    "    print (\"Finding nearest neighbour for test words\")\n",
    "    for test_word in test_words:\n",
    "        test_label = kmeans.predict(np.array(reduced_vector[word_list.index(test_word)]).reshape(1,-1))\n",
    "        min_dist = float(\"inf\")\n",
    "        close_word = 'NONE'\n",
    "        cluster_words = word_clusters[test_label[0]]\n",
    "        for cluster_word in cluster_words:\n",
    "            dist = wc.cosine_dist(reduced_vector[word_list.index(test_word)],\n",
    "                                  reduced_vector[word_list.index(cluster_word)])\n",
    "            if (dist < min_dist) and (cluster_word != test_word):\n",
    "                min_dist = dist\n",
    "                close_word = cluster_word\n",
    "        print (str(test_word) + \" is closest to \" + str(close_word))\n",
    "\n",
    "    print (\"Completed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Brown corpus words\n",
      "Get common words and vocabulary words\n",
      "Loading dictionary from NPY file\n",
      "Computing Probability of C given W\n",
      "Computing Probability of C\n",
      "Building PHI vector for each vocab word\n",
      "Converting dictionary to list\n",
      "Applying PCA\n",
      "Applying K Means Clustering\n",
      "Grouping vocabulary based on labels\n",
      "Finding nearest neighbour for test words\n",
      "communism is closest to frontier\n",
      "autumn is closest to attacks\n",
      "cigarette is closest to ice\n",
      "pulmonary is closest to artery\n",
      "mankind is closest to survival\n",
      "africa is closest to asia\n",
      "chicago is closest to boston\n",
      "revolution is closest to german\n",
      "september is closest to july\n",
      "chemical is closest to oxygen\n",
      "detergent is closest to foam\n",
      "dictionary is closest to text\n",
      "storm is closest to weekend\n",
      "worship is closest to intellectual\n",
      "Completed..\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
